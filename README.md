# Matrix Calculus for Machine Learning and Beyond

This is the course page for an **18.063 Matrix Calculus** at MIT taught in **January 2025** ([IAP](https://elo.mit.edu/iap/)) by
Professors [Alan Edelman](https://math.mit.edu/~edelman/) and [Steven G. Johnson](https://math.mit.edu/~stevenj/).

* For past versions of this course, see [Matrix Calculus in IAP 2023 (OCW)](https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/) on OpenCourseWare (also [on github](https://github.com/mitmath/matrixcalc/tree/iap2023), with videos [on YouTube](https://www.youtube.com/playlist?list=PLUl4u3cNGP62EaLLH92E_VCN4izBKK6OE)).  See also [Matrix Calculus in IAP 2022 (OCW)](https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2022/pages/lecture-notes-and-readings/) (also [on github](https://github.com/mitmath/matrixcalc/tree/iap2022)), and [Matrix Calculus 2024 (github)](https://github.com/mitmath/matrixcalc/tree/iap2024); the previous years used the temporary 18.S096 "special subject" course number.

**Lectures:** MWF time 11am–1pm, Jan 13–Jan 31 in room 4-370; lecture recordings to be posted (MIT only).  3 units, *2 problem sets* due Jan 24 and Feb 31 — submitted electronically [via Canvas](https://canvas.mit.edu/courses/29776), no exams.  TA/grader: TBD.

**Course Notes**: [Draft notes from IAP 2024](https://www.dropbox.com/scl/fi/iq4plt8oqja845cuuosa4/Matrix-Calculus-latest.pdf?rlkey=nsnytdu28jje41nhh1bl2dbba&st=i6lfha0r&dl=0).  Other materials to be posted.

**Piazza forum:** Online discussions at [Piazza](https://piazza.com/mit/spring2025/18063).

**Description:**

> We all know that calculus courses such as 18.01 and 18.02 are univariate and vector calculus, respectively. Modern applications such as machine learning and large-scale optimization require the next big step, "matrix calculus" and calculus on arbitrary vector spaces.
>
> This class covers a coherent approach to matrix calculus showing techniques that allow you to think of a matrix holistically (not just as an array of scalars), generalize and compute derivatives of important matrix factorizations and many other complicated-looking operations, and understand how differentiation formulas must be re-imagined in large-scale computing. We will discuss reverse/adjoint/backpropagation differentiation, custom vector-Jacobian products, and how modern automatic differentiation is more computer science than calculus (it is neither symbolic formulas nor finite differences).

**Prerequisites:** Linear Algebra such as [18.06](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/) and multivariate calculus such as [18.02](https://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/).

Course will involve simple numerical computations using the [Julia language](https://github.com/mitmath/julia-mit).   Ideally install it on your own computer following [these instructions](https://github.com/mitmath/julia-mit#installing-julia-and-ijulia-on-your-own-computer), but as a fallback you can run it in the cloud here:
[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/mitmath/binder-env/main)

**Topics:**

Here are some of the planned topics:

* Derivatives as linear operators and linear approximation on arbitrary vector spaces: beyond gradients and Jacobians.
* Derivatives of functions with matrix inputs and/or outputs (e.g. matrix inverses and determinants).  Kronecker products and matrix "vectorization".
* Derivatives of matrix factorizations (e.g. eigenvalues/SVD) and derivatives with constraints (e.g. orthogonal matrices).
* Multidimensional chain rules, and the significance of right-to-left ("forward") vs. left-to-right ("reverse") composition.  Chain rules on computational graphs (e.g. neural networks).
* Forward- and reverse-mode manual and automatic multivariate differentiation.
* Adjoint methods (vJp/pullback rules) for derivatives of solutions of linear, nonlinear, and differential equations.
* Application to nonlinear root-finding and optimization.  Multidimensional Newton and steepest–descent methods.
* Applications in engineering/scientific optimization and machine learning.
* Second derivatives, Hessian matrices, quadratic approximations, and quasi-Newton methods.

## Lecture 1 (Jan 13)

* part 1: overview ([slides](https://docs.google.com/presentation/d/16uwYARbg4unaGU4Enp6uQvlBb6N21j1UINQW99om6R4/edit?usp=sharing))
* part 2: derivatives as linear operators: matrix functions, gradients, product and chain rule
* [video (MIT only)](https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=87d1cb6c-c91a-4f4a-a573-b2630084197e)

 Re-thinking derivatives as linear operators: f(x+dx)-f(x)=df=f′(x)[dx] — f′ is the [linear operator](https://en.wikipedia.org/wiki/Linear_map) that gives the change df in the *output* from a "tiny" change dx in the *inputs*, to *first order* in dx (i.e. dropping higher-order terms).   When we have a vector function f(x)∈ℝᵐ of vector inputs x∈ℝⁿ, then f'(x) is a linear operator that takes n inputs to m outputs, which we can think of as an m×n matrix called the [Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant) (typically covered only superficially in 18.02).

 In the same way, we can define derivatives of matrix-valued operators as linear operators on matrices.  For example, f(X)=X² gives f'(X)[dX] = X dX + dX X.  Or f(X) = X⁻¹ gives f'(X)[dX] = –X⁻¹ dX X⁻¹.   These are perfectly good linear operators acting on matrices dX, even though they are not written in the form (Jacobian matrix)×(column vector)!   (We *could* rewrite them in the latter form by reshaping the inputs dX and the outputs df into column vectors, more formally by choosing a basis, and we will later cover how this process can be made more elegant using [Kronecker products](https://en.wikipedia.org/wiki/Kronecker_product).  But for the most part it is neither necessary nor desirable to express all linear operators as Jacobian matrices in this way.)

 Reviewed the (easy) derivations of the sum rule d(f+g)=df+dg and the product rule d(fg) = (df)g+f(dg), directly from the definition of f(x+dx)-f(x)=df=f′(x)[dx], dropping higher-order terms.

 Discussed the chain rule for f(g(x)) (f'(x)=g'(h(x))h'(x), where this is a *composition* of two linear operations, performing h' then g' — g'h' ≠ h'g'!).   For functions from vectors to vectors, the chain rule is simply the *product of Jacobians*.   Moreover, as soon as you compose 3 or more functions, it can a make a huge difference whether you multiply the Jacobians from left-to-right ("reverse-mode", or "backpropagation", or "adjoint differentiation") or right-to-left ("forward-mode"). Showed, for example, that if you have *many inputs but a single output* (as is common in machine learning and other types of optimization problem), that it is vastly more efficient to multiply left-to-right than right-to-left, and such "backpropagation algorithms" are a key factor in the practicality of large-scale optimization.

**Further reading**: *Draft Course Notes* (link above), chapters 1 and 2.
 [matrixcalculus.org](http://www.matrixcalculus.org/) (linked in the slides) is a fun site to play with derivatives of matrix and vector functions.  The [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) has a lot of formulas for these derivatives, but no derivations.  Some [notes on vector and matrix differentiation](https://cdn-uploads.piazza.com/paste/j779e63owl53k6/04b2cb8c2f300212d723bea822a6b856085b28e28ca9debc75a05761a436499c/6.S087_Lecture_2.pdf) were posted for 6.S087 from IAP 2021.

 **Further reading (gradients)**: A fancy name for a row vector is a "covector" or [linear form](https://en.wikipedia.org/wiki/Linear_form), and the fancy version of the relationship between row and column vectors is the [Riesz representation theorem](https://en.wikipedia.org/wiki/Riesz_representation_theorem), but until you get to non-Euclidean geometry you may be happier thinking of a row vector as the transpose of a column vector.

**Further reading (chain rule)**: The terms "forward-mode" and "reverse-mode" differentiation are most prevalent in [automatic differentiation (AD)](https://en.wikipedia.org/wiki/Automatic_differentiation), which will will cover later in this course. You can find many, many articles online about [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) in neural networks.   There are many other versions of this, e.g. in differential geometry the derivative linear operator (multiplying Jacobians and perturbations dx right-to-left) is called a [pushforward](https://en.wikipedia.org/wiki/Pushforward_(differential)), whereas multiplying a gradient row vector (covector) by a Jacobian left-to-right is called a [pullback](https://en.wikipedia.org/wiki/Pullback_(differential_geometry)).   This [video on the principles of AD in Julia](https://www.youtube.com/watch?v=UqymrMG-Qi4) by [Dr. Mohamed Tarek](https://github.com/mohamed82008) also starts with a similar left-to-right (reverse) vs right-to-left (forward) viewpoint and goes into how it translates to Julia code, and how you define custom chain-rule steps for Julia AD.  In other fields, "reverse mode" is sometimes called an "adjoint method": see the [notes on adjoint methods](https://github.com/mitmath/18335/blob/spring21/notes/adjoint/adjoint.pdf) and [slides](https://github.com/mitmath/18335/blob/spring21/notes/adjoint/adjoint-intro.pdf) from 18.335 ([video](https://mit.zoom.us/rec/share/xLxMyhBSoIhSFxce5lHb1ubItby5BKFs6mgJJ7kMmjotETmaYm4YA22TA8w8n13i.6sTEFrkkloG7LFeR?startTime=1619463273000)).

**Further reading (fancier math)**: the perspective of derivatives as linear operators is sometimes called a [Fréchet derivative](https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative) and you can find lots of very abstract (what I'm calling "fancy") presentations of this online, chock full of weird terminology whose purpose is basically to generalize the concept to weird types of vector spaces.  The "little-o notation" o(δx) we're using here for "infinitesimal asymptotics" is closely related to the [asymptotic notation](https://en.wikipedia.org/wiki/Big_O_notation) used in computer science, but in computer science people are typically taking the limit as the argument (often called "n") becomes very *large* instead of very small.

## Lecture 2 (Jan 15)

* part 1: matrix Jacobians via [vectorization](https://en.wikipedia.org/wiki/Vectorization_(mathematics)); notes: [2×2 Matrix Jacobians (html)](https://rawcdn.githack.com/mitmath/matrixcalc/3f6758996e40c5c1070279f89f7f65e76e08003d/notes/2x2Jacobians.jl.html) [(pluto notebook source code)](https://github.com/mitmath/matrixcalc/blob/main/notes/2x2Jacobians.jl) [(jupyter notebook)](notes/2x2Jacobians.ipynb).  Course notes: chapter 3.

* example of reverse-mode/adjoint differentiation for optimizing g(p) = f(A(p)⁻¹b), showing why a linear-operator formula like d(A⁻¹)=–A⁻¹ dA A⁻¹ is actually perfectly practical and usable, and why evaluating the chain rule outputs-to-inputs is so practically important.  Last few slides of lecture 1.

* [video (MIT only)](https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=0abdc678-c7bd-4253-9e11-b263008420eb)


**Further reading**: Wikipedia has a useful list of [properties of the matrix trace](https://en.wikipedia.org/wiki/Trace_(linear_algebra)#Properties).  The "matrix dot product" introduced today is also called the [Frobenius inner product](https://en.wikipedia.org/wiki/Frobenius_inner_product), and the corresponding norm ("length" of the matrix viewed as a vector) is the [Frobenius norm](https://mathworld.wolfram.com/FrobeniusNorm.html).   When you "flatten" a matrix A by stacking its columns into a single vector, the result is called [vec(A)](https://en.wikipedia.org/wiki/Vectorization_(mathematics)), and many important linear operations on matrices can be expressed as [Kronecker products](https://en.wikipedia.org/wiki/Kronecker_product).  The [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) has lots of formulas for derivatives of matrix functions.  See the [notes on adjoint methods](https://github.com/mitmath/18335/blob/spring21/notes/adjoint/adjoint.pdf) and [slides](https://github.com/mitmath/18335/blob/spring21/notes/adjoint/adjoint-intro.pdf) from 18.335 ([video](https://mit.zoom.us/rec/share/xLxMyhBSoIhSFxce5lHb1ubItby5BKFs6mgJJ7kMmjotETmaYm4YA22TA8w8n13i.6sTEFrkkloG7LFeR?startTime=1619463273000)).

## Lecture 3 (Jan 17)

* generalized gradients and inner products — [handwritten notes](https://www.dropbox.com/scl/fi/byg5mpcnnk4xh9tqjbjmk/Inner-Products-and-Norms.pdf?rlkey=egsdhyee9go9v17iuxxqx1edj&dl=0) and course notes chapter 5
    - also norms and derivatives: why a norm of the input and output are needed to *define* a derivative
    - more on handling units: when the components of the vector are quantities different units, defining the inner product (and hence the norm) requires dimensional weight factors to scale the quantities.  (Using standard gradient / inner product implicitly uses weights given by whatever units you are using.) A change of variables (to nondimensionalize the problem) is equivalent (for steepest descent) to a nondimensionalization of the inner-product/norm, but the former is typically easier for use with off-the-shelf optimization software.   Usually, you want to use units/scaling so that all your quantities have similar scales, otherwise steepest descent may converge very slowly!

* The [gradient of the determinant](https://rawcdn.githack.com/mitmath/matrixcalc/b08435612045b17745707f03900e4e4187a6f489/notes/determinant_and_inverse.html) is ∇(det A) = det(A)A⁻ᵀ (course notes chapter 7)

* an amazing trick by [Mathias (1996)](https://doi.org/10.1137/S0895479895283409): `f([A dA; 0I A]) = [f(A) f′(A)[dA]; 0I f(A)]` (in Julia notation) for any analytic/smooth function f(A) acting on square matrices.  (e.g. matrix powers/polynomials, matrix exponentials, etcetera).

* [video (MIT only)](https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=134a6f3a-b1b9-442e-8e53-b26300842887)

* [pset 1](psets/pset1.pdf) (due Feb 24)

Generalizing **gradients** to *scalar* functions f(x) for x in arbitrary *vector spaces* x ∈ V.   The key thing is that we need not just a vector space, but an **inner product** x⋅y (a "dot product", also denoted ⟨x,y⟩ or ⟨x|y⟩); V is then formally called a [Hilbert space](https://en.wikipedia.org/wiki/Hilbert_space).   Then, for *any* scalar function, since df=f'(x)[dx] is a linear operator mapping dx∈V to scalars df∈ℝ (a "[linear form](https://en.wikipedia.org/wiki/Linear_form)"), it turns out that it [*must* be a dot product](https://en.wikipedia.org/wiki/Riesz_representation_theorem) of dx with "something", and we call that "something" the gradient!  That is, once we define a dot product, then for any scalar function f(x) we can define ∇f by f'(x)[dx]=∇f⋅dx.  So ∇f is always something with the same "shape" as x (the [steepest-ascent](https://math.stackexchange.com/questions/223252/why-is-gradient-the-direction-of-steepest-ascent) direction).

Talked about the general [requirements for an inner product](https://en.wikipedia.org/wiki/Inner_product_space): linearity, positivity, and (conjugate) symmetry (and also mentioned the [Cauchy–Schwarz inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality), which follows from these properties).  Gave some examples of inner products, such as the familiar Euclidean inner product xᵀy or a weighted inner product.  Defined the most obvious inner product of m×n matrices: the [Frobenius inner product](https://en.wikipedia.org/wiki/Frobenius_inner_product) A⋅B=`sum(A .* B)`=trace(AᵀB)=vec(A)ᵀvec(B), the sum of the products of the matrix entries.  This also gives us the "Frobenius norm" ‖A‖²=A⋅A=trace(AᵀA)=‖vec(A)‖², the square root of the sum of the squares of the entries.   Using this, we can now take the derivatives of various scalar functions of matrices, e.g. we considered

* f(A)=tr(A) ⥰ ∇f = I
* f(A)=‖A‖ ⥰ ∇f = A/‖A‖
* f(A)=xᵀAy ⥰ ∇f = xyᵀ (for constant x, y)
* f(A)=det(A) ⥰ ∇f = det(A)(A⁻¹)ᵀ = transpose of the [adjugate](https://en.wikipedia.org/wiki/Adjugate_matrix) of A

Also talked about the definition of a [norm](https://en.wikipedia.org/wiki/Norm_(mathematics)) (which can be obtained from an inner product if you have one, but can also be defined by itself), and why a norm is necessary to define a derivative: it is embedded in the definition of what a higher-order term o(δx) means.   (Although there are many possible norms, [in finite dimensions all norms are equivalent up to constant factors](https://math.mit.edu/~stevenj/18.335/norm-equivalence.pdf), so the definition of a derivative does not depend on the choice of norm.)

Made precise and derived (with the help of Cauchy–Schwarz) the well known fact that ∇f is the **steepest-ascent** direction, for *any* scalar-valued function on a vector space with an inner product (any Hilbert space), in the norm corresponding to that inner product.  That is, if you take a step δx with a fixed length ‖δx‖=s, the greatest increase in f(x) to first order is found in a direction parallel to ∇f.

Closed with a sketch of an amazing formula by Mathias for the derivatives of smooth functions from square matrices to square matrices, which you will investigate more for homework: for a sufficiently smooth function f(A) from square matrices to square matrices, it turns out that:
```math
f(\begin{bmatrix} A & \delta A \\ & A \end{bmatrix}) =
    \begin{bmatrix} f(A) & f'(A)[\delta A] \\ & f(A) \end{bmatrix} \, .
```
(This is *exact* for any δA, even if it is not small!)  You will investigate this more closely for homework.

**Further reading (∇det)**: Course notes, chapter 7.  There are lots of discussions of the
[derivative of a determinant](https://en.wikipedia.org/wiki/Jacobi%27s_formula) online, involving the ["adjugate" matrix](https://en.wikipedia.org/wiki/Adjugate_matrix) det(A)A⁻¹.
Not as well documented is that the gradient of the determinant is the cofactor matrix widely used for the [Laplace expansion](https://en.wikipedia.org/wiki/Laplace_expansion) of a determinant.
The formula for the [derivative of log(det A)](https://statisticaloddsandends.wordpress.com/2018/05/24/derivative-of-log-det-x/) is also nice, and logs of determinants appear in surprisingly many applications (from statistics to quantum field theory).  The [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) contains many of these formulas, but no derivations.   A nice application of d(det(A)) is solving for eigenvalues λ by applying Newton's method to det(A-λI)=0, and more generally one can solve det(M(λ))=0 for any function Μ(λ) — the resulting roots λ are called [nonlinear eigenvalues](https://en.wikipedia.org/wiki/Nonlinear_eigenproblem) (if M is nonlinear in λ), and one can [apply Newton's method](https://www.maths.manchester.ac.uk/~ftisseur/talks/FT_talk2.pdf) using the determinant-derivative formula here.

**Further reading (Mathias formula):** The earliest derivation of this formula seems to be by [Mathias (1996)](https://doi.org/10.1137/S0895479895283409).  A generalization is proved in [Higham (2008)](https://epubs.siam.org/doi/book/10.1137/1.9780898717778): if $A$ is an $n \times n$ matrix, the function $f(A)$ need only be $2n-1$ times differentiable (or even only once differentiable if $A$ is [normal](https://en.wikipedia.org/wiki/Normal_matrix)), instead of requiring it to be [analytic](https://en.wikipedia.org/wiki/Analytic_function) (have a Taylor series).  The Mathias formula is very closely related to the power law and other rules for [2×2 Jordan blocks / generalized eigenvectors of defective matrices](https://web.mit.edu/18.06/www/Spring17/jordan-vectors.pdf), as well as to the [matrix representation of dual numbers](https://en.wikipedia.org/wiki/Dual_number#Matrix_representation).

## Lecture 4 (Jan 22)

* part 1: forward-mode automatic differentiation (AD) via [dual numbers](https://en.wikipedia.org/wiki/Dual_number) ([Julia notebook](notes/AutoDiff.ipynb)) - course notes, chapter 8
* part 2: reverse-mode differentiation examples - course notes, section 6.3
    - slides on nonlinear root-finding, optimization, and adjoint-method differentiation [slides](https://docs.google.com/presentation/d/1U1lB5bhscjbxEuH5FcFwMl5xbHl0qIEkMf5rm0MO8uE/edit?usp=sharing) - talked about reverse-mode "implicit differentiation" involving [implicit functions](https://en.wikipedia.org/wiki/Implicit_function) defined by solutions of nonlinear systems of equations
    - Reverse-mode gradients for neural networks: [handwritten backpropagation notes](https://www.dropbox.com/scl/fi/bke4pbr342e1jhv9qytg1/NN-Backpropagation.pdf?rlkey=b7krtzdt4hgsj63zyq9ok2gqv&dl=0)
* [video (MIT only)](https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=dd303e51-c198-4637-97fe-b26a008418c0)

**Further reading on forward AD**: Course notes, chapter 8.  Googling "automatic differentiation" will turn up many, many resources — this is a huge practical field these days.   [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) (described detail by [this paper](https://arxiv.org/abs/1607.07892)) in Julia uses [dual number](https://en.wikipedia.org/wiki/Dual_number) arithmetic similar to lecture to compute derivatives; see also this [AMS review article](http://www.ams.org/publicoutreach/feature-column/fc-2017-12), or google "dual number automatic differentiation" for many other reviews.    Adrian Hill posted some nice [lecture notes on automatic differentiation (Julia-based)](https://adrhill.github.io/julia-ml-course/L6_Automatic_Differentiation/) for an ML course at TU Berlin (Summer 2023).  [TaylorDiff.jl](https://github.com/JuliaDiff/TaylorDiff.jl) extends this to higher-order derivatives.

**Further reading on adjoint methods** Course notes, section 6.3. A useful review of topology-optimization methods in engineering (where "every pixel/voxel" of a structure is a degree of freedom) can be found in [Sigmund and Maute (2013)](https://link.springer.com/article/10.1007/s00158-013-0978-6).  See the [notes on adjoint methods](https://github.com/mitmath/18335/blob/spring21/notes/adjoint/adjoint.pdf) and [slides](https://github.com/mitmath/18335/blob/spring21/notes/adjoint/adjoint-intro.pdf) from 18.335 ([video](https://mit.zoom.us/rec/share/xLxMyhBSoIhSFxce5lHb1ubItby5BKFs6mgJJ7kMmjotETmaYm4YA22TA8w8n13i.6sTEFrkkloG7LFeR?startTime=1619463273000)).   "Implicit" differentiation, for passing the chain rule through implicit functions (usually in reverse mode) via the [implicit function theorem](https://en.wikipedia.org/wiki/Implicit_function_theorem), has become an increasingly prominent topic in AD; see e.g. the paper ["Efficient and modular implicit differentiation" (Blondel et al., 2021)](https://arxiv.org/abs/2105.15183), and the [ImplicitDifferentiation.jl package](https://github.com/JuliaDecisionFocusedLearning/ImplicitDifferentiation.jl) in Julia.   A non-obvious example of an implicit function is the solution of a "nested" optimization problem x(p) = argminₓ g(p,x), called a [bilevel optimization problem](https://en.wikipedia.org/wiki/Bilevel_optimization), which (locally) satisfies the nonlinear equations ∇ₓg=0 (or more generally the [KKT conditions](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions)), and these can be differentiated via the implicit function; see e.g. [Amos and Kolter (2017)](https://arxiv.org/abs/1703.00443).

**Further reading on backpropagation for NNs**: [Strang (2019)](https://math.mit.edu/~gs/learningfromdata/) section VII.3 and [18.065 OCW lecture 27](https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/resources/lecture-27-backpropagation-find-partial-derivatives/). You can find many, many articles online about [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) in neural networks. Backpropagation for neural networks is closely related to [backpropagation/adjoint methods for recurrence relations (course notes)](https://math.mit.edu/~stevenj/18.336/recurrence2.pdf), and on [computational graphs (blog post)](https://colah.github.io/posts/2015-08-Backprop/).  We will return to computational graphs in a future lecture.

## Lecture 5 (Jan 24)
* part 0: interpreting the Kronecker product A₃⊗A₂⊗A₁ of *three* matrices as a linear operator Y = (A₃⊗A₂⊗A₁)[X] mapping "3d arrays" X to 3d arrays Y.  It is the ["tensor contraction"](https://en.wikipedia.org/wiki/Tensor_contraction) operation $Y[i_1,i_2,i_3] = \sum_{j_1,j_2,j_3} A_1[i_1,j_1] A_2[i_2,j_2] A_3[i_3,j_3] X[j_1,j_2,j_3]$, which is equivalent to the ["vectorized"](https://en.wikipedia.org/wiki/Vectorization_%28mathematics%29) matrix–vector operation vec(Y)=(A₃⊗A₂⊗A₁)vec(X), and can also be viewed as acting the 3 matrices along the "3 directions" of X.
* part 1: forward and reverse-mode automatic differentiation on computational graphs: course notes section 8.3
* part 2: differentiation of ODE solutions (forward mode): course notes chapter 9, [double-pendulum example notebook](https://github.com/mitmath/matrixcalc/blob/main/notes/double-pendulum-sensitivity.ipynb) ([Wikipedia](https://en.wikipedia.org/wiki/Double_pendulum) has some nice double-pendulum animations)
* [video (MIT only)](https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=b777873f-f6a0-4309-bf7d-b26a00842046)
* [pset 1 solutions](psets/pset1sol.pdf) and [Julia notebook](psets/pset1sol.ipynb)
* [pset 2](psets/pset2.pdf): due Friday January 31 at midnight

**Further reading (AD on graphs):** Course notes, section 8.3.  See [Prof. Edelman's poster](notes/backprop_poster.pdf) about backpropagation on graphs, this blog post on [calculus on computational graphs](https://colah.github.io/posts/2015-08-Backprop/) for a gentle review, and these Columbia [course notes](http://www.cs.columbia.edu/~mcollins/ff2.pdf) for a more formal approach.  Implementing automatic reverse-mode AD is much more complicated than defining a new number type, unfortunately, and involves a lot more intricacies of compiler technology.  See also Chris Rackauckas's blog post on [tradeoffs in AD](https://www.stochasticlifestyle.com/engineering-trade-offs-in-automatic-differentiation-from-tensorflow-and-pytorch-to-jax-and-julia/), and Chris's discussion post on [AD limitations](https://discourse.julialang.org/t/open-discussion-on-the-state-of-differentiable-physics-in-julia/72900/2).

**Further reading (ODEs):** Course notes, chapter 9. A classic reference on adjoint-method (reverse-mode/backpropagation) differentiation of ODEs (and generalizations thereof), using notation similar to that used today, is [Cao et al (2003)](https://epubs.siam.org/doi/10.1137/S1064827501380630) ([pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.455&rep=rep1&type=pdf)).  See also the [SciMLSensitivity.jl package](https://docs.sciml.ai/SciMLSensitivity/stable/) for sensitivity analysis with Chris Rackauckas's amazing [DifferentialEquations.jl software suite](https://diffeq.sciml.ai/stable/) for numerical solution of ODEs in Julia, along with his [notes from 18.337](https://rawcdn.githack.com/mitmath/18337/7b0e890e1211bfa253782f7862389aeaa321e8d7/lecture11/adjoints.html).  There is a nice YouTube [lecture on adjoint sensitivity of ODEs](https://www.youtube.com/watch?v=k6s2G5MZv-I), again using a similar notation.   A *discrete* version of this process is [adjoint methods for recurrence relations](https://math.mit.edu/~stevenj/18.336/recurrence2.pdf) (MIT course notes), in which case one obtains a reverse-order "adjoint" recurrence relation.

## Lecture 6 (Jan 27)
* part 1: differentiation of ODE solutions (reverse mode, for scalar functions G of the solution): course notes chapter 9, [double-pendulum example notebook](https://github.com/mitmath/matrixcalc/blob/main/notes/double-pendulum-sensitivity.ipynb) ([Wikipedia](https://en.wikipedia.org/wiki/Double_pendulum) has some nice double-pendulum animations)
* part 2: forward and reverse-mode automatic differentiation on computational graphs: course notes chapter 8
* [video link (MIT only)](https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=0199e13d-4812-49d7-9a4a-b27100840437)

**Further reading**: See the further reading from lecture 5.  You can find many discussions of Diract delta functions online and in textbooks — as a starting point, here are [Prof. Johnson's notes on delta functions and distributions](https://math.mit.edu/~stevenj/18.303/delta-notes.pdf), aimed at an elementary-calculus level.


## Lecture 7 (Jan 29)
* part 1: more AD on graphs, this time re-visiting the implicit differentiation from lecture 4 (same result, different perspective).
* part 2: Second derivatives, Hessian matrices, quadratic approximations — notes, chapter 12 — and combinations of reverse and forward mode (notes, section 8.4.1).
* [video link (MIT only)](https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=2b297d48-a286-4549-b531-b27100840bb0)

**Further reading (second derivatives)**:
* [Bilinear forms](https://en.wikipedia.org/wiki/Bilinear_form) are an important generalization of quadratic operations to arbitrary vector spaces, and we saw that the second derivative can be viewed as a [symmetric bilinear forms](https://en.wikipedia.org/wiki/Symmetric_bilinear_form).   This is closely related to a [quadratic form](https://en.wikipedia.org/wiki/Quadratic_form), which is just what we get by plugging in the same vector twice, e.g. the f''(x)[δx,δx]/2 that appears in quadratic approximations for f(x+δx) is a quadratic form.  The most familar multivariate version of f''(x) is the [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix); Khan academy has an elementary [introduction to quadratic approximation](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/quadratic-approximations/a/quadratic-approximation)
* [Positive-definite](https://en.wikipedia.org/wiki/Definite_matrix) Hessian matrices, or more generally [definite quadratic forms](https://en.wikipedia.org/wiki/Definite_quadratic_form) f″, appear at extrema (f′=0) of scalar-valued functions f(x) that are local minima; there a lot [more formal treatments](http://www.columbia.edu/~md3405/Unconstrained_Optimization.pdf) of the same idea, and conversely Khan academy has the [simple 2-variable version](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/second-partial-derivative-test) where you can check the sign of the 2×2 eigenvalues just by looking at the determinant and a single entry (or the trace).  There's a nice [stackexchange discussion](https://math.stackexchange.com/questions/2285282/relating-condition-number-of-hessian-to-the-rate-of-convergence) on why an [ill-conditioned](https://nhigham.com/2020/03/19/what-is-a-condition-number/) Hessian tends to make steepest descent converge slowly; some Toronto [course notes on the topic](https://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/slides/lec07.pdf) may also be helpful.
* See e.g. these Stanford notes on [sequential quadratic optimization](https://web.stanford.edu/class/ee364b/lectures/seq_notes.pdf) using trust regions (sec. 2.2).  See 18.335 [notes on BFGS quasi-Newton methods](https://github.com/mitmath/18335/blob/spring21/notes/BFGS.pdf) (also [video](https://mit.zoom.us/rec/share/naqcRgSkZ0VNeDp0ht8QmB566mPowuHJ8k0LcaAmZ7XxaCT1ch4j_O4Khzi-taXm.CXI8xFthag4RvvoC?startTime=1620241284000)).   The fact that a quadratic optimization problem in a sphere has [strong duality](https://en.wikipedia.org/wiki/Strong_duality) and hence is efficiently solvable is discussed in section 5.2.4 of the [*Convex Optimization* book](https://web.stanford.edu/~boyd/cvxbook/).  There has been a lot of work on [automatic Hessian computation](https://en.wikipedia.org/wiki/Hessian_automatic_differentiation), but for large-scale problems you can ultimately only compute Hessian–vector products efficiently in general, which are equivalent to a directional derivative of the gradient, and can be used e.g. for [Newton–Krylov methods](https://en.wikipedia.org/wiki/Newton%E2%80%93Krylov_method).

## Lecture 8 (Jan 31)
* part 1: more AD on graphs, this time re-visiting the differentiation of ODEs.
* part 2: derivatives with constraints, derivatives of eigenproblems [(html)](https://rawcdn.githack.com/mitmath/matrixcalc/d11b747d70a5d9e1a3da8cdb68a7f8a220d3afae/notes/symeig.jl.html) [(julia source)](notes/symeig.jl) — course notes, chapter 13
* [video link (MIT only)](https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=16c81f60-25b4-4e53-b1cd-b271008412ff)

**Further reading (transposes of derivatives)**: In the graph picture, applying reverse-mode differentiation to ODEs involved "transposing" the derivative of the ODE constraint ∂u/∂t – f(u,p,t) = 0, and in particular "transposing" the linear operator ∂/∂t on functions.  Linear algebra applied to differential operators is important to the study of partial differential equatios, e.g. in 18.303, 18.101 (functional analysis), and many other courses (e.g. quantum mechanics 8.04+ is taught this way, where the "transpose" is denoted by the "dagger" operation †). See also my [notes on differential operators for 18.06](http://web.mit.edu/18.06/www/Fall07/operators.pdf) from Fall 2007 which gives an informal introduction to the "transpose of a derivative" (denoted by a superscript "H" rather than "T" to include the complex case).

**Further reading (derivatives with constraints and eigenproblems)**: Computing derivatives on curved surfaces ("manifolds") is closely related to [tangent spaces](https://en.wikipedia.org/wiki/Tangent_space) in differential geometry.   The effect of constraints can also be expressed in terms of [Lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier), which are useful in expressing optimization problems with constraints (see also chapter 5 of [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/) by Boyd and Vandenberghe).
In physics, first and second derivatives of eigenvalues and first derivatives of eigenvectors are often presented as part of ["time-independent" perturbation theory](https://en.wikipedia.org/wiki/Perturbation_theory_(quantum_mechanics)#Time-independent_perturbation_theory) in quantum mechanics, or as the [Hellmann–Feynmann theorem](https://en.wikipedia.org/wiki/Hellmann%E2%80%93Feynman_theorem) for the case of dλ.    The derivative of an eigenvector involves *all* of the other eigenvectors, but a much simpler "vector–Jacobian product" (involving only a single eigenvector and eigenvalue) can be obtained from left-to-right differentiation of a *scalar function* of an eigenvector, as reviewed in the [18.335 notes on adjoint methods](https://github.com/mitmath/18335/blob/spring21/notes/adjoint/adjoint.pdf).

* When differentiating eigenvalues λ of matrices A(x), a complication arises at eigenvalue crossings (multiplicity k > 1), where in general the eigenvalues (and eigenvectors) cease to be differentiable.  (More generally, this problem arises for any [implicit function](https://en.wikipedia.org/wiki/Implicit_function) with a repeated root.)  In this case, one option is to use an expanded definition of sensitivity analysis called a **generalized gradient** (a k×k *matrix-valued* linear operator G(x)\[dx\] whose *eigenvalues* are the perturbations dλ); see e.g. [Cox (1995)](https://doi.org/10.1006/jfan.1995.1117), [Seyranian *et al.* (1994)](https://doi.org/10.1007/BF01742705), and [Stechlinski (2022)](https://doi.org/10.1016/j.laa.2022.04.019). (Physicists call this idea [degenerate perturbation theory](https://ocw.mit.edu/courses/8-06-quantum-physics-iii-spring-2018/a0889c5ca8a479c3e56c544d646fb770_MIT8_06S18ch1.pdf).) A recent formulation of similar ideas is called a **lexicographic directional derivative**; see [Nesterov (2005)](https://doi.org/10.1007/s10107-005-0633-0) and [Barton *et al* (2017)](https://doi.org/10.1080/10556788.2017.1374385). Sometimes, optimization problems involving eigenvalues can be reformulated to avoid this difficulty by using [SDP](https://en.wikipedia.org/wiki/Semidefinite_programming) constraints [(Men *et al.*, 2014)](http://doi.org/10.1364/OE.22.022632).  For a [defective matrix](https://en.wikipedia.org/wiki/Defective_matrix) the situation is worse: even the generalized derivatives blow up, because dλ is proportional to the *square root* of the perturbation ‖dA‖.

## Other Topics in Differentiation

There are many topics that we did not have time to cover, even in 16 hours of lectures.  (We didn't even cover all of the course notes!)  If you came into this class thinking that taking derivatives is easy and you already learned everything there is to know about it in first-year calculus, hopefully we've convinced you that it is an enormously rich subject that is impossible to exhaust in a single course.  Some of the things it might have been nice to include are:

* Limitations of AD, and how to handle them.  When AD hits something it can't handle (or handles inefficiently), you may have to write a custom Jacobian–vector product (a "Jvp", "frule", or "pushforward") in forward mode, and/or a custom rowvector–Jacobian product (a "vJp", "rrule", "pullback", or Jacobianᵀ–vector product) in reverse mode.   In Julia with Zygote AD, this is done using [the ChainRules packages](https://github.com/JuliaDiff/ChainRulesCore.jl).  In Python with JAX, this is done with [`jax.custom_jvp`](https://jax.readthedocs.io/en/latest/_autosummary/jax.custom_jvp.html) and/or [`jax.custom_vjp`](https://jax.readthedocs.io/en/latest/_autosummary/jax.custom_vjp.html), respectively.  In principle this is straightforward, but the APIs can take some getting used to because of the generality that they support.
* When we have a function f(u) mapping *functions* u(x) to *scalars* f(u) (called a "functional"), we can still define a gradient ∇f — this leads us to the important subject of ["calculus of variations"](https://en.wikipedia.org/wiki/Calculus_of_variations): see the course notes chapter 12 12 and references therein.
*  When computing derivatives of *complex* functions f(z), the derivative is defined the same way, but many functions you might want to differentiate in practice (e.g. *all* functions you might want to optimize, which must be real-valued) are *not differentiable* according to the ordinary definition.  In order to deal with this, we *extend* the definition of derivative to something called **CR calculus**, which can handle many more functions.  See the [handwritten notes](https://www.dropbox.com/scl/fi/cn2cmzf1q2anpeeg9s5a4/CR-Calculus.pdf?rlkey=b0qd9b3r8ldc9tu0s80nxpvir&dl=0)
and [lecture video (MIT only)](https://mit.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=d7475b0d-cc1f-4bee-9f35-b1050083ee17) on this from 18.063 in IAP 2024. A well-known reference on CR calculus can be found in the UCSD notes [The Complex Gradient Operator and the CR-Calculus](https://arxiv.org/abs/0906.4835) by Ken Kreutz-Delgado (2009).  These are also called [Wirtinger derivatives](https://en.wikipedia.org/wiki/Wirtinger_derivatives).  There is some support for this in automatic differentiations packages, e.g. see the documentation on [complex functions in ChainRules.jl](https://juliadiff.org/ChainRulesCore.jl/dev/maths/complex.html) or [complex functions in JAX](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#complex-numbers-and-differentiation).   The special case of "[holomorphic](https://en.wikipedia.org/wiki/Holomorphic_function)" / "[analytic](https://en.wikipedia.org/wiki/Analytic_function)" functions where we have an "ordinary" derivative (= linear operator on dz) is the main topic of [complex analysis](https://en.wikipedia.org/wiki/Complex_analysis), for which there are many resources (textbooks, online tutorials, and classes like [18.04](https://ocw.mit.edu/courses/18-04-complex-variables-with-applications-spring-2018/)).
* Many, many more derivative results for matrix functions and factorizations can be found in the literature, some of them quite tricky to derive.  For example, a number of references are listed in [this github issue for the ChainRules package](https://github.com/JuliaDiff/ChainRules.jl/issues/117).
* Another important generalization of differential calculus is to derivatives on curved manifolds and differential geometry, leading to the [exterior derivative](https://en.wikipedia.org/wiki/Exterior_derivative).
* Various generalizations to "directional derivatives", "Gateaux derivatives", and more, allow one to handle more cases where conventional (Frechét) differentiation fails.  See, for example, the *further reading* links for eigenvalue crossings and multiple roots in Lecture 7, above.
* Famous generalizations of differentiation are the ["distributional"](https://en.wikipedia.org/wiki/Distributional_derivative) and ["weak"](https://en.wikipedia.org/wiki/Weak_derivative) derivatives, for example to obtain [Dirac delta "functions"](https://en.wikipedia.org/wiki/Dirac_delta_function) by differentiating discontinuities. This requires changing not only the definition of "derivative" but also *changing the definition of "function"*, as reviewed in [these MIT course notes](https://math.mit.edu/~stevenj/18.303/delta-notes.pdf).
